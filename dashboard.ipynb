{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd3e30b-e096-429a-9b38-953f56164638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aec6f8d-21bf-4fbd-88f2-2021e8211ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010a8bc8-8dc8-434f-b20e-0b0f25298209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_attention(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, prob=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Automatically choose device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.rnn = nn.LSTM(input_size, hidden_dim, n_layers, dropout=prob, batch_first=True)\n",
    "\n",
    "        # Attention \n",
    "\t\t\n",
    "        # 1st Layer: Input = Queries + Hidden State\n",
    "        self.linear1 = nn.Linear(2*hidden_dim,hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        # 2nd Layer: Output = 1 (Sum of the Normalized weights * keys)\n",
    "        self.linear2 = nn.Linear(hidden_dim,1)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "        # Fully-connected output layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "\n",
    "        # Move all layers to the selected device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x, lengths, h0=None):\n",
    "\n",
    "        lengths = [min(l, 35) for l in lengths]\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "\n",
    "        x = x.to(self.device)\n",
    "        if h0 is not None:\n",
    "            h0 = (h0[0].to(self.device), h0[1].to(self.device))\n",
    "\n",
    "        # Compute the RNN output (sequence of states for the whole input) h to hl\n",
    "\n",
    "        r_out, _ = self.rnn(x, h0)\n",
    "\n",
    "        # Query (Should be Hidden State l before junk tokens)\n",
    "\n",
    "        queries = torch.stack([r_out[d, lengths[d] - 1, :] for d in range(batch_size)])\n",
    "        keys = r_out\n",
    "\n",
    "        queries = queries.unsqueeze(1)\n",
    "        queries_expanded = queries.expand(-1, keys.size(1), -1) # Shape: (N, S, H)\n",
    "        queries_flat = queries_expanded.reshape(-1, keys.size(2))\n",
    "        keys_flat = keys.reshape(-1, keys.size(2))\n",
    "        inputs = torch.cat([queries_flat, keys_flat], dim=1)\n",
    "\n",
    "        unnormalized_weights = self.tanh(self.linear1(inputs))\n",
    "        unnormalized_weights = self.linear2(unnormalized_weights).squeeze(-1) # Now shape is (N*S,)\t\t\n",
    "        unnormalized_weights = unnormalized_weights.view(batch_size, seq_length) # Now shape is (N,S)\n",
    "\n",
    "        mask = torch.arange(seq_length, device=self.device).unsqueeze(0) >= torch.tensor(lengths, device=self.device).unsqueeze(1)\n",
    "        unnormalized_weights.masked_fill_(mask, float('-inf'))\n",
    "     \n",
    "        alphas = self.softmax(unnormalized_weights)\n",
    "        alphas_expanded = alphas.unsqueeze(-1).expand(-1, -1, keys.size(2))\n",
    "\n",
    "        c = torch.sum(alphas_expanded * keys, dim=1)\n",
    "        output = self.logsoftmax(self.fc1(self.dropout(c)))\n",
    "\n",
    "        return output, alphas\n",
    "\n",
    "\n",
    "class RNN_attention_with_train(RNN_attention):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, prob=0.0,\n",
    "                 batch_size=50, lr=0.0005, saved_files='./saved_models/RNN_sentiment_analysis'):\n",
    "        super().__init__(input_size, output_size, hidden_dim, n_layers, prob)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.loss_during_training = []\n",
    "        self.valid_loss_during_training = []\n",
    "        self.batch_size = batch_size\n",
    "        os.makedirs(os.path.dirname(saved_files), exist_ok=True)\n",
    "        self.saved_files = saved_files\n",
    "\n",
    "   \n",
    "    def predict_proba(self, docs, lengths, Y=None):\n",
    "        accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            x_input = torch.stack([torch.tensor(d, dtype=torch.float32) for d in docs]).to(self.device)\n",
    "            logprobs, weights = self.forward(x_input, lengths)\n",
    "            logprobs = logprobs.cpu().detach().numpy()\n",
    "\n",
    "            if Y is not None and len(Y) > 0:\n",
    "                accuracy = np.sum(np.argmax(logprobs, 1) == Y) / np.shape(Y)[0]\n",
    "\n",
    "        return logprobs, accuracy\n",
    "\n",
    "\n",
    "    def fit(self, docs_train, docs_val, Y, Yval, len_train, len_val, epochs=100, print_every=5):\n",
    "        self.print_every = print_every\n",
    "        self.epochs = epochs\n",
    "        self.num_train = len(docs_train)\n",
    "        self.num_batchs = int(np.floor(self.num_train / self.batch_size))\n",
    "        self.num_val = len(docs_val)\n",
    "        self.num_batchs_val = int(np.floor(self.num_val / self.batch_size))\n",
    "    \n",
    "        labels = torch.tensor(Y, dtype=torch.long, device=self.device)\n",
    "        labelsval = torch.tensor(Yval, dtype=torch.long, device=self.device)\n",
    "    \n",
    "        for e in range(self.epochs):\n",
    "            self.train()\n",
    "            idx = np.random.permutation(self.num_train)\n",
    "            running_loss = 0.\n",
    "            start_time = time.time()\n",
    "    \n",
    "            for i in range(self.num_batchs):\n",
    "                self.optim.zero_grad()\n",
    "                idx_batch = idx[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                idx_batch = [d for d in idx_batch if len_train[d] > 0]\n",
    "    \n",
    "                # Convert docs to tensors\n",
    "                x_input = torch.stack([torch.tensor(docs_train[d], dtype=torch.float32) for d in idx_batch]).to(self.device)\n",
    "                out, _ = self.forward(x_input, [len_train[d] for d in idx_batch])\n",
    "                loss = self.criterion(out, labels[idx_batch])\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
    "                self.optim.step()\n",
    "    \n",
    "            self.loss_during_training.append(running_loss / self.num_batchs)\n",
    "            torch.save(self.state_dict(), self.saved_files + f'_epoch_{e}.pth')\n",
    "    \n",
    "            # Validation\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                running_loss = 0.\n",
    "                idx = np.random.permutation(self.num_val)\n",
    "                for i in range(self.num_batchs_val):\n",
    "                    idx_batch = idx[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                    idx_batch = [d for d in idx_batch if len_val[d] > 0]\n",
    "    \n",
    "                    # Convert docs to tensors for validation\n",
    "                    x_input = torch.stack([torch.tensor(docs_val[d], dtype=torch.float32) for d in idx_batch]).to(self.device)\n",
    "                    out, _ = self.forward(x_input, [len_val[d] for d in idx_batch])\n",
    "                    loss = self.criterion(out, labelsval[idx_batch])\n",
    "                    running_loss += loss.item()\n",
    "    \n",
    "                self.valid_loss_during_training.append(running_loss / self.num_batchs_val)\n",
    "    \n",
    "            if (e+1) % self.print_every == 0:\n",
    "                print(f\"============================ Epoch: {e + 1}, Time: {time.time() - start_time} ============================\")\n",
    "                print(f\"Training Loss = {self.loss_during_training[-1]}  |  Validation Loss = {self.valid_loss_during_training[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa64aad-641d-426c-9aae-2455a2aad9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load(\"fasttext.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcbab72-548c-4727-9989-fd4a0b148ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\",\"ner\"])\n",
    "def preprocess(text):\n",
    "    # Delete expressions\n",
    "    text = re.sub(r\"http\\S+|@\\S+\",\"\", text)\n",
    "    # Turn into lowecases\n",
    "    text = text.lower()\n",
    "    # SpaCy preprocessing\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "        and token.is_alpha\n",
    "    ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "664f50d8-154a-4280-a1da-47e00e6d3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_embeddings(model, sentence, garbage_token, maxlen=35):\n",
    "    vector_size = model.vector_size\n",
    "    garbage_vector = model.wv[garbage_token.text] if garbage_token.text in model.wv else np.zeros(vector_size)\n",
    "\n",
    "    sentence_vectors = [model.wv[word] for word in sentence]\n",
    "\n",
    "    if len(sentence_vectors) < maxlen:\n",
    "        pad_len = maxlen - len(sentence_vectors)\n",
    "        sentence_vectors.extend([garbage_vector] * pad_len)\n",
    "    else:\n",
    "        sentence_vectors = sentence_vectors[:maxlen]\n",
    "        \n",
    "\n",
    "    return np.array(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1db1d24-41a8-4f9f-b035-eb854815348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iqbal\\AppData\\Local\\Temp\\ipykernel_20268\\747756171.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fasttext_RNN.load_state_dict(torch.load('fasttextmodel.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_RNN = RNN_attention_with_train(input_size=200, output_size=6, hidden_dim=64, n_layers=4)\n",
    "fasttext_RNN.load_state_dict(torch.load('fasttextmodel.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4548d410-9b0c-48c8-be63-7690ce892630",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_sentiment = {\n",
    "    0: 'Sadness',\n",
    "    1: 'Joy',\n",
    "    2: 'Love',\n",
    "    3: 'Anger',\n",
    "    4: 'Fear',\n",
    "    5: 'Surprise'\n",
    "}\n",
    "\n",
    "garbage_token = nlp(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42ba62c2-fcc5-44a9-bf47-929f1c010f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return None\n",
    "    elif isinstance(val, list):\n",
    "        return val\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "num_topics = len(df_topic_text[\"Topic Distribution\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d17c0571-71ef-4afd-b8f4-d0e68093d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "redditData = pd.read_csv(\"redditDataInference.csv\", parse_dates=[\"date\"], index_col=0)\n",
    "df_topic_text = pd.read_csv(\"reddit_lda_results.csv\", index_col=0)\n",
    "df_topic_text[\"Topic Distribution\"] = df_topic_text[\"Topic Distribution\"].apply(safe_parse)\n",
    "\n",
    "topic_keywords = pd.read_csv(\"topic_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45446d7f-b39d-48c5-8bc8-0eb71989cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tokens by sentiment\n",
    "emotion_words = defaultdict(list)\n",
    "\n",
    "for _, row in redditData.iterrows():\n",
    "    if isinstance(row['tokens_english'], str):\n",
    "        tokens = eval(row['tokens'])\n",
    "    else:\n",
    "        tokens = row['tokens']\n",
    "    emotion_words[row['sentiment']].extend(tokens)\n",
    "\n",
    "sentiment_options = list(emotion_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbb91a1-040d-4cd4-b4c2-1663dcebf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud_topic(topic_id, topic_keywords):\n",
    "\n",
    "    if topic_id in topic_keywords['Topic'].values:\n",
    "        topic_row = topic_keywords[topic_keywords['Topic'] == topic_id]\n",
    "\n",
    "        if not topic_row.empty:\n",
    "            word_columns = [col for col in topic_row.columns if col.startswith('Word_')]\n",
    "            words = topic_row[word_columns].iloc[0].values\n",
    "\n",
    "            word_freq = {word: (len(words) - i) for i, word in enumerate(words) if pd.notna(word)}\n",
    "\n",
    "            if not word_freq:\n",
    "                print(f\"No valid words found for topic ID {topic_id}. Cannot generate wordcloud.\")\n",
    "                return None\n",
    "\n",
    "            wc = WordCloud(width=600, height=400, background_color='white', collocations=False).generate_from_frequencies(word_freq)\n",
    "\n",
    "            img = BytesIO()\n",
    "            plt.figure(figsize=(8, 5)) # Adjusted figure size\n",
    "\n",
    "            wordcloud_image_array = wc.to_array()\n",
    "            plt.imshow(wordcloud_image_array, interpolation='bilinear')\n",
    "\n",
    "            plt.axis('off') # Hide axes\n",
    "            plt.tight_layout(pad=0) # Reduced padding around the plot\n",
    "            plt.savefig(img, format='png')\n",
    "\n",
    "            plt.close()\n",
    "            img.seek(0)\n",
    "\n",
    "            return base64.b64encode(img.getvalue()).decode()\n",
    "        else:\n",
    "            print(f\"Topic ID {topic_id} found in values but no corresponding row.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Topic ID {topic_id} not found in topic_keywords.\")\n",
    "        return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13ee9765-3bdc-46b7-83ab-f8e3628ae07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8051/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x19d5164dc70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "external_stylesheets = ['https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css']\n",
    "custom_color_palette = px.colors.qualitative.Bold\n",
    "\n",
    "colors = {\n",
    "    'background': '#f9f9f9',\n",
    "    'card_background': '#ffffff',\n",
    "    'primary': '#4361ee',\n",
    "    'secondary': '#3f37c9',\n",
    "    'text': '#333333',\n",
    "    'border': '#e0e0e0',\n",
    "    'highlight': '#4cc9f0',\n",
    "    'sadness': '#4895ef',\n",
    "    'joy': '#ffd166',\n",
    "    'love': '#06d6a0',\n",
    "    'anger': '#ef476f',\n",
    "    'fear': '#9d4edd',\n",
    "    'surprise': '#48cae4'\n",
    "}\n",
    "\n",
    "emotion_colors = {\n",
    "    'sadness': colors['sadness'],\n",
    "    'joy': colors['joy'],\n",
    "    'love': colors['love'],\n",
    "    'anger': colors['anger'],\n",
    "    'fear': colors['fear'],\n",
    "    'surprise': colors['surprise']\n",
    "}\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "card_style = {\n",
    "    'backgroundColor': colors['card_background'],\n",
    "    'padding': '15px',\n",
    "    'marginBottom': '15px'\n",
    "}\n",
    "\n",
    "section_header_style = {\n",
    "    'backgroundColor': colors['primary'],\n",
    "    'color': 'white',\n",
    "    'padding': '10px'\n",
    "}\n",
    "\n",
    "button_style = {\n",
    "    'backgroundColor': colors['primary'],\n",
    "    'color': 'white',\n",
    "    'border': 'none',\n",
    "    'padding': '8px 12px'\n",
    "}\n",
    "\n",
    "app.layout = html.Div([\n",
    "    # Header\n",
    "    html.Div([\n",
    "        html.H1(\"Sentiment Analysis & Topic Modeling Dashboard\", \n",
    "                style={'textAlign': 'center', 'color': colors['text']}),\n",
    "        html.P(\"Analyze emotions and topics from Reddit data\",\n",
    "               style={'textAlign': 'center', 'color': colors['secondary']})\n",
    "    ]),\n",
    "    \n",
    "    # PART 1: SENTIMENT CLASSIFIER\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.I(className=\"fas fa-heart\", style={'marginRight': '10px'}),\n",
    "            html.H2(\"Part 1: Sentiment Classifier\")\n",
    "        ], style=section_header_style),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Div([\n",
    "                html.H3(\"Analyze Your Text\", style={'color': colors['primary']}),\n",
    "                html.P(\"Enter a sentence to classify its emotional sentiment\"),\n",
    "                \n",
    "                dcc.Input(\n",
    "                    id='input-sentence',\n",
    "                    type='text',\n",
    "                    placeholder='Enter a sentence...',\n",
    "                    style={'width': '70%', 'padding': '8px'}\n",
    "                ),\n",
    "                \n",
    "                html.Button(\n",
    "                    'Classify', \n",
    "                    id='classify-button', \n",
    "                    n_clicks=0,\n",
    "                    style=button_style\n",
    "                ),\n",
    "                \n",
    "                html.Div(id='output-label', style={\n",
    "                    'marginTop': '15px',\n",
    "                    'padding': '10px',\n",
    "                    'backgroundColor': colors['highlight'] + '20'\n",
    "                }),\n",
    "            ]),\n",
    "            \n",
    "            html.Div([\n",
    "                html.H3(\"Sentiment Attention Weights\", style={'color': colors['primary']}),\n",
    "                html.P(\"Visualize how the model weighs different words in your input\"),\n",
    "                \n",
    "                dcc.Graph(\n",
    "                    id='attention-weights-graph',\n",
    "                    figure=go.Figure().update_layout(\n",
    "                        template='plotly_white',\n",
    "                        plot_bgcolor=colors['card_background'],\n",
    "                        paper_bgcolor=colors['card_background']\n",
    "                    )\n",
    "                )\n",
    "            ])\n",
    "        ], style=card_style)\n",
    "    ]),\n",
    "    \n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.I(className=\"fas fa-chart-pie\", style={'marginRight': '10px'}),\n",
    "            html.H2(\"Part 2: Reddit Emotion Analysis\")\n",
    "        ], style=section_header_style),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Div([\n",
    "                html.H3(\"Select Date Range\", style={'color': colors['primary']}),\n",
    "                html.P(\"Filter Reddit data by date range\"),\n",
    "                \n",
    "                dcc.DatePickerRange(\n",
    "                    id='date-picker',\n",
    "                    min_date_allowed=redditData['date'].min().date(),\n",
    "                    max_date_allowed=redditData['date'].max().date(),\n",
    "                    start_date=redditData['date'].min().date(),\n",
    "                    end_date=redditData['date'].max().date(),\n",
    "                    updatemode='bothdates'\n",
    "                ),\n",
    "            ]),\n",
    "            \n",
    "            html.Div([\n",
    "                html.Div([\n",
    "                    dcc.Graph(\n",
    "                        id='emotion-pie',\n",
    "                        figure=px.pie(title='Emotion Distribution').update_layout(\n",
    "                            template='plotly_white',\n",
    "                            plot_bgcolor=colors['card_background'],\n",
    "                            paper_bgcolor=colors['card_background']\n",
    "                        )\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block'}),\n",
    "                \n",
    "                html.Div([\n",
    "                    dcc.Graph(\n",
    "                        id='emotion-bar',\n",
    "                        figure=px.bar(title='Emotion Counts').update_layout(\n",
    "                            template='plotly_white',\n",
    "                            plot_bgcolor=colors['card_background'],\n",
    "                            paper_bgcolor=colors['card_background']\n",
    "                        )\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'marginLeft': '4%'})\n",
    "            ]),\n",
    "            \n",
    "            html.Div([\n",
    "                html.H3(\"Sample Post\", style={'color': colors['primary']}),\n",
    "                html.P(\"Click on an emotion in the pie chart to see a random post with that emotion\"),\n",
    "                \n",
    "                html.Div(id='random-text', style={\n",
    "                    'padding': '10px',\n",
    "                    'border': f'1px solid {colors[\"border\"]}',\n",
    "                    'backgroundColor': colors['highlight'] + '10'\n",
    "                })\n",
    "            ]),\n",
    "            \n",
    "            html.Div([\n",
    "                html.H3(\"Emotion Trends Over Time\", style={'color': colors['primary']}),\n",
    "                \n",
    "                dcc.Graph(\n",
    "                    id='emotion-time-series',\n",
    "                    figure=px.line(title='Emotion Trends Over Time (Weekly)').update_layout(\n",
    "                        template='plotly_white',\n",
    "                        plot_bgcolor=colors['card_background'],\n",
    "                        paper_bgcolor=colors['card_background']\n",
    "                    )\n",
    "                )\n",
    "            ]),\n",
    "            \n",
    "            html.Div([\n",
    "                html.H3(\"Word Clouds by Emotion\", style={'color': colors['primary']}),\n",
    "                html.P(\"Select an emotion to view its most common words\"),\n",
    "                \n",
    "                dcc.Dropdown(\n",
    "                    id='sentiment-dropdown',\n",
    "                    options=[{'label': s.capitalize(), 'value': s} for s in sentiment_options],\n",
    "                    value=sentiment_options[0],\n",
    "                    clearable=False,\n",
    "                    style={'width': '200px'}\n",
    "                ),\n",
    "                \n",
    "                html.Div([\n",
    "                    html.Img(\n",
    "                        id='wordcloud-image',\n",
    "                        style={'maxWidth': '100%'}\n",
    "                    )\n",
    "                ], style={'textAlign': 'center'})\n",
    "            ])\n",
    "        ], style=card_style)\n",
    "    ]),\n",
    "    \n",
    "    # PART 3: TOPIC MODELING\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.I(className=\"fas fa-comments\", style={'marginRight': '10px'}),\n",
    "            html.H2(\"Part 3: Topic Modeling\")\n",
    "        ], style=section_header_style),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Div([\n",
    "                html.H3(\"Topic Distribution Overview\", style={'color': colors['primary']}),\n",
    "                html.P(\"Histogram showing the distribution of predominant topics across documents\"),\n",
    "                \n",
    "                dcc.Graph(\n",
    "                    figure=px.histogram(\n",
    "                        df_topic_text,\n",
    "                        x=\"Document Index\",\n",
    "                        color=\"Predominant Topic\",\n",
    "                        title=\"Predominant LDA Topic Distribution\",\n",
    "                        color_discrete_sequence=custom_color_palette,\n",
    "                        category_orders={\"Predominant Topic\": sorted(df_topic_text[\"Predominant Topic\"].unique())},\n",
    "                    ) if not df_topic_text.empty else go.Figure().update_layout(\n",
    "                        title=\"Predominant LDA Topic Distribution (No data)\"\n",
    "                    ).update_layout(\n",
    "                        template='plotly_white',\n",
    "                        plot_bgcolor=colors['card_background'],\n",
    "                        paper_bgcolor=colors['card_background']\n",
    "                    )\n",
    "                )\n",
    "            ]),\n",
    "            \n",
    "            html.Div([\n",
    "                html.H3(\"Document Explorer\", style={'color': colors['primary']}),\n",
    "                html.P(\"Select a document index to view its details\"),\n",
    "                \n",
    "                html.Div([\n",
    "                    dcc.Input(\n",
    "                        id='document-index',\n",
    "                        type='number',\n",
    "                        min=0,\n",
    "                        max=len(df_topic_text)-1,\n",
    "                        value=0,\n",
    "                        style={'width': '100px', 'padding': '8px'}\n",
    "                    ),\n",
    "                    \n",
    "                    html.Button(\n",
    "                        'Load Document Info',\n",
    "                        id='submit-button',\n",
    "                        n_clicks=0,\n",
    "                        style=button_style\n",
    "                    )\n",
    "                ], style={'marginBottom': '15px'}),\n",
    "                \n",
    "                html.Div([\n",
    "                    html.Div([\n",
    "                        html.H4(\"Document Information\", style={'color': colors['primary']}),\n",
    "                        html.Div(\n",
    "                            id='document-info',\n",
    "                            style={'padding': '10px', 'border': f'1px solid {colors[\"border\"]}'}\n",
    "                        )\n",
    "                    ], style={'width': '48%', 'display': 'inline-block'}),\n",
    "                    \n",
    "                    html.Div([\n",
    "                        html.H4(\"Document Text\", style={'color': colors['primary']}),\n",
    "                        html.Div(\n",
    "                            id='document-text',\n",
    "                            style={'padding': '10px', 'border': f'1px solid {colors[\"border\"]}', 'maxHeight': '200px', 'overflow': 'auto'}\n",
    "                        )\n",
    "                    ], style={'width': '48%', 'display': 'inline-block', 'marginLeft': '4%'})\n",
    "                ]),\n",
    "                \n",
    "                html.Div([\n",
    "                    html.H4(\"Topic WordCloud\", style={'color': colors['primary']}),\n",
    "                    html.P(\"Visual representation of keywords for the selected topic\"),\n",
    "                    \n",
    "                    html.Div([\n",
    "                        html.Img(\n",
    "                            id='wordcloud-image-topic',\n",
    "                            style={'maxWidth': '100%'}\n",
    "                        )\n",
    "                    ], style={'textAlign': 'center'})\n",
    "                ])\n",
    "            ])\n",
    "        ], style=card_style)\n",
    "    ]),\n",
    "    \n",
    "], style={\n",
    "    'fontFamily': 'Arial, sans-serif',\n",
    "    'maxWidth': '1200px',\n",
    "    'margin': '0 auto',\n",
    "    'padding': '20px',\n",
    "    'backgroundColor': colors['background'],\n",
    "    'color': colors['text']\n",
    "})\n",
    "\n",
    "@app.callback(\n",
    "    [Output('output-label', 'children'),\n",
    "     Output('attention-weights-graph', 'figure')],\n",
    "    Input('classify-button', 'n_clicks'),\n",
    "    State('input-sentence', 'value')\n",
    ")\n",
    "def classify_and_plot(n_clicks, sentence):\n",
    "    if not sentence:\n",
    "        return \"Please enter a sentence.\", go.Figure().update_layout(\n",
    "            title=\"Attention weights will appear here\",\n",
    "            template='plotly_white',\n",
    "            plot_bgcolor=colors['card_background'],\n",
    "            paper_bgcolor=colors['card_background']\n",
    "        )\n",
    "\n",
    "    tokens = preprocess(sentence)\n",
    "    inputs = get_fasttext_embeddings(model, tokens, garbage_token)\n",
    "\n",
    "    x_input = torch.Tensor([[w for w in inputs]])\n",
    "    log_probs, attention_weights_tensor = fasttext_RNN(x_input, [len(tokens)])\n",
    "    predicted_labels = torch.argmax(log_probs, dim=1).cpu().numpy()\n",
    "    sentiment = label_to_sentiment[predicted_labels[0]]\n",
    "\n",
    "    sentiment_color = emotion_colors.get(sentiment, colors['primary'])\n",
    "\n",
    "    attention_weights_np = attention_weights_tensor.squeeze().cpu().detach().numpy()\n",
    "    x1 = list(range(len(attention_weights_np)))\n",
    "    plot_title = \" \".join(token for token in tokens if token != \"#\")\n",
    "\n",
    "    fig = px.line(\n",
    "        x=x1,\n",
    "        y=attention_weights_np,\n",
    "        markers=True,\n",
    "        labels={'x': 'Token Index', 'y': 'Attention Weight'},\n",
    "        title=f\"Attention Weights for: {plot_title}\"\n",
    "    )\n",
    "    fig.update_traces(marker=dict(color=sentiment_color), \n",
    "                      line=dict(color=sentiment_color))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        plot_bgcolor=colors['card_background'],\n",
    "        paper_bgcolor=colors['card_background']\n",
    "    )\n",
    "\n",
    "    return html.Div([\n",
    "        html.I(className=\"fas fa-info-circle\", style={'marginRight': '10px'}),\n",
    "        f\"Predicted Sentiment: \",\n",
    "        html.Span(sentiment.capitalize(), style={'color': sentiment_color})\n",
    "    ]), fig\n",
    "\n",
    "@app.callback(\n",
    "    [Output('emotion-pie', 'figure'),\n",
    "     Output('emotion-bar', 'figure'),\n",
    "     Output('random-text', 'children'),\n",
    "     Output('emotion-time-series', 'figure')],\n",
    "    [Input('date-picker', 'start_date'),\n",
    "     Input('date-picker', 'end_date'),\n",
    "     Input('emotion-pie', 'clickData')]\n",
    ")\n",
    "def update_graphs(start_date, end_date, clickData):\n",
    "    if not start_date or not end_date:\n",
    "        empty_pie = px.pie(title='Emotion Distribution').update_layout(\n",
    "            template='plotly_white',\n",
    "            plot_bgcolor=colors['card_background'],\n",
    "            paper_bgcolor=colors['card_background']\n",
    "        )\n",
    "        empty_bar = px.bar(title='Emotion Counts').update_layout(\n",
    "            template='plotly_white',\n",
    "            plot_bgcolor=colors['card_background'],\n",
    "            paper_bgcolor=colors['card_background']\n",
    "        )\n",
    "        empty_line = px.line(title='Emotion Trends Over Time').update_layout(\n",
    "            template='plotly_white',\n",
    "            plot_bgcolor=colors['card_background'],\n",
    "            paper_bgcolor=colors['card_background']\n",
    "        )\n",
    "        return empty_pie, empty_bar, \"Please select a date range\", empty_line\n",
    "    \n",
    "    mask = (redditData['date'] >= start_date) & (redditData['date'] <= end_date)\n",
    "    filtered = redditData.loc[mask]\n",
    "    \n",
    "    emotion_order = list(emotion_colors.keys())\n",
    "    filtered = filtered[filtered['sentiment'].isin(emotion_order)]\n",
    "    \n",
    "    pie_fig = px.pie(\n",
    "        filtered,\n",
    "        names='sentiment',\n",
    "        title='Emotion Distribution',\n",
    "        color='sentiment',\n",
    "        color_discrete_map=emotion_colors,\n",
    "        hole=0.4\n",
    "    )\n",
    "    pie_fig.update_traces(textinfo='percent+label')\n",
    "    pie_fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        plot_bgcolor=colors['card_background'],\n",
    "        paper_bgcolor=colors['card_background']\n",
    "    )\n",
    "\n",
    "    emotion_counts = filtered['sentiment'].value_counts().reset_index()\n",
    "    emotion_counts.columns = ['Emotion', 'Count']\n",
    "    \n",
    "    emotion_counts['Emotion'] = pd.Categorical(\n",
    "        emotion_counts['Emotion'],\n",
    "        categories=emotion_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    emotion_counts = emotion_counts.sort_values('Emotion')\n",
    "\n",
    "    bar_fig = px.bar(\n",
    "        emotion_counts,\n",
    "        x='Emotion',\n",
    "        y='Count',\n",
    "        color='Emotion',\n",
    "        title='Emotion Counts',\n",
    "        color_discrete_map=emotion_colors,\n",
    "        text='Count'\n",
    "    )\n",
    "    \n",
    "    bar_fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "    bar_fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        plot_bgcolor=colors['card_background'],\n",
    "        paper_bgcolor=colors['card_background'],\n",
    "        xaxis_title=None,\n",
    "        yaxis_title=\"Count\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    filtered.loc[:, 'period'] = pd.to_datetime(filtered['date'].dt.to_period('2W').apply(lambda r: r.start_time))\n",
    "    time_series_data = filtered.groupby(['period', 'sentiment']).size().reset_index(name='count')\n",
    "    \n",
    "    line_fig = px.line(\n",
    "        time_series_data,\n",
    "        x='period',\n",
    "        y='count',\n",
    "        color='sentiment',\n",
    "        title='Emotion Trends Over Time (Weekly)',\n",
    "        color_discrete_map=emotion_colors,\n",
    "        markers=True\n",
    "    )\n",
    "    line_fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        plot_bgcolor=colors['card_background'],\n",
    "        paper_bgcolor=colors['card_background'],\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Count\",\n",
    "        legend_title=\"Emotion\"\n",
    "    )\n",
    "\n",
    "    sample_text = \"\"\n",
    "    if clickData:\n",
    "        emotion = clickData['points'][0]['label']\n",
    "        matching_rows = filtered[filtered['sentiment'] == emotion]\n",
    "        if not matching_rows.empty:\n",
    "            random_text = random.choice(matching_rows['translated_text'].tolist())\n",
    "            emoji_map = {\n",
    "                'sadness': 'ðŸ˜¢',\n",
    "                'joy': 'ðŸ˜Š',\n",
    "                'love': 'â¤ï¸',\n",
    "                'anger': 'ðŸ˜¡',\n",
    "                'fear': 'ðŸ˜¨',\n",
    "                'surprise': 'ðŸ˜²'\n",
    "            }\n",
    "            emoji = emoji_map.get(emotion, 'ðŸ’¬')\n",
    "            emotion_color = emotion_colors.get(emotion, colors['primary'])\n",
    "            \n",
    "            sample_text = html.Div([\n",
    "                html.Div([\n",
    "                    html.Span(f\"{emoji} Random '{emotion}' post:\", \n",
    "                              style={'color': emotion_color})\n",
    "                ]),\n",
    "                html.Div(random_text)\n",
    "            ])\n",
    "        else:\n",
    "            sample_text = f\"No posts with emotion '{emotion}' in this range.\"\n",
    "\n",
    "    return pie_fig, bar_fig, sample_text, line_fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('wordcloud-image', 'src'),\n",
    "    Input('sentiment-dropdown', 'value')\n",
    ")\n",
    "def update_wordcloud(sentiment):\n",
    "    if not sentiment or sentiment not in emotion_words:\n",
    "        return \"\"\n",
    "    \n",
    "    text = ' '.join(emotion_words[sentiment])\n",
    "    \n",
    "    color_func = lambda *args, **kwargs: emotion_colors.get(sentiment, colors['primary'])\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        color_func=color_func,\n",
    "        max_words=100\n",
    "    ).generate(text)\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    wordcloud.to_image().save(buffer, format='PNG')\n",
    "    img_b64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "    return f\"data:image/png;base64,{img_b64}\"\n",
    "\n",
    "@app.callback(\n",
    "    [Output('document-info', 'children'),\n",
    "     Output('document-text', 'children'),\n",
    "     Output('wordcloud-image-topic', 'src')],\n",
    "    [Input('submit-button', 'n_clicks')],\n",
    "    [State('document-index', 'value')]\n",
    ")\n",
    "def update_document_info(n_clicks, document_index):\n",
    "    if df_topic_text.empty:\n",
    "        return [\n",
    "            html.Div(\"No topic data loaded.\")\n",
    "        ], [\n",
    "            html.Div(\"No text available.\")\n",
    "        ], \"\"\n",
    "\n",
    "    if document_index is None or document_index < 0 or document_index >= len(df_topic_text):\n",
    "        return [\n",
    "            html.Div(\"Invalid document index\", style={'color': 'red'})\n",
    "        ], [\n",
    "            html.Div(\"No text available\")\n",
    "        ], \"\"\n",
    "\n",
    "    doc_row = df_topic_text.iloc[document_index]\n",
    "    doc_info = [\n",
    "        html.Div([\n",
    "            html.Span(\"Document Index: \"),\n",
    "            html.Span(f\"{doc_row['Document Index']}\")\n",
    "        ]),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Span(\"Predominant Topic: \"),\n",
    "            html.Span(f\"{doc_row['Predominant Topic']}\")\n",
    "        ]),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Span(\"Topic Distribution: \"),\n",
    "            html.Span(f\"{doc_row.get('Topic Distribution', 'N/A')}\")\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    topic_num = doc_row['Predominant Topic']\n",
    "    topic_color = custom_color_palette[topic_num % len(custom_color_palette)] if len(custom_color_palette) > 0 else colors['primary']\n",
    "    \n",
    "    doc_text = html.Div([\n",
    "        html.Div(doc_row.get('Text', 'Text not available'), \n",
    "                 style={\n",
    "                     'backgroundColor': f\"{topic_color}10\",\n",
    "                     'borderLeft': f\"4px solid {topic_color}\"\n",
    "                 })\n",
    "    ])\n",
    "    \n",
    "    wordcloud_src = generate_wordcloud_topic(doc_row['Predominant Topic'], topic_keywords)\n",
    "    \n",
    "    if wordcloud_src:\n",
    "        wordcloud_img = f\"data:image/png;base64,{wordcloud_src}\"\n",
    "    else:\n",
    "        wordcloud_img = \"\"\n",
    "        \n",
    "    return doc_info, doc_text, wordcloud_img\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,port=8051)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
